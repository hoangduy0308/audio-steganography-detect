{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d86ed3",
   "metadata": {},
   "source": [
    "# Huấn luyện mô hình CNN cho phát hiện Steganography âm thanh\n",
    "\n",
    "Notebook này thực hiện quá trình tải dữ liệu âm thanh, tiền xử lý (tạo Mel-spectrogram), áp dụng SpecAugment, xây dựng và huấn luyện mô hình CNN để phân loại âm thanh sạch và âm thanh giấu tin (stego). Cuối cùng, nó đánh giá mô hình và lưu lại các kết quả. Đây là phiên bản khởi tạo của mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b207eb",
   "metadata": {},
   "source": [
    "## 1. Cài đặt và Imports thư viện\n",
    "\n",
    "Trước khi chạy, đảm bảo bạn đã cài đặt tất cả các thư viện cần thiết. Bạn có thể chạy dòng sau nếu thiếu (bỏ dấu `#`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy librosa matplotlib tensorflow scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix, classification_report\n",
    "import random\n",
    "import sys\n",
    "import subprocess\n",
    "print(\"Các thư viện đã được import thành công!\")\n",
    "try:\n",
    "    import resampy  # type: ignore\n",
    "except ImportError:\n",
    "    print('Đang cài đặt resampy...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', 'resampy'])\n",
    "    import resampy  # type: ignore\n",
    "GLOBAL_SEED = 2025\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "print(f'Đã thiết lập GLOBAL_SEED = {GLOBAL_SEED}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae5c83",
   "metadata": {},
   "source": [
    "## 2. Cấu hình GPU\n",
    "\n",
    "Thiết lập cấu hình GPU để TensorFlow có thể sử dụng nếu có sẵn. Điều này giúp tăng tốc độ huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"Không tìm thấy GPU vật lý. TensorFlow sẽ chạy trên CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0dda4",
   "metadata": {},
   "source": [
    "## 3. Cấu hình chung và Tham số\n",
    "\n",
    "Thiết lập các hằng số và tham số quan trọng cho quá trình tiền xử lý dữ liệu và huấn luyện mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn đến dataset\nDATA_PARENT_DIR = '/kaggle/input/data-lsb/data'\n\n# Tên file processed data (lưu tại working dir)\nPROCESSED_DATA_FILE = 'audio_data.npz'\n\nSAMPLE_RATE = 22050\nN_FFT = 2048\nHOP_LENGTH = 512\nN_MELS = 128\nFIXED_DURATION_SEC = 4\nEXPECTED_SPECTROGRAM_COLS = int(np.ceil(FIXED_DURATION_SEC * SAMPLE_RATE / HOP_LENGTH))\n\n# Tham số cho mô hình\nIMG_WIDTH, IMG_HEIGHT = EXPECTED_SPECTROGRAM_COLS, N_MELS\nINPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 1)\nBATCH_SIZE = 8\nEPOCHS = 150\n\n# --- ĐƯỜNG DẪN LƯU OUTPUT ---\nWORKING_DIR = '/kaggle/working/'\nMODEL_SAVE_DIR = os.path.join(WORKING_DIR, 'model_output')\nRESULT_IMAGE_DIR = os.path.join(WORKING_DIR, 'result_images')\n\nprint('Các tham số cấu hình đã được đặt:')\nprint(f'  Kích thước spectrogram mong đợi: {IMG_HEIGHT}x{IMG_WIDTH}')\nprint(f'  Thư mục dataset gốc: {DATA_PARENT_DIR}')\nprint(f'  File dữ liệu xử lý: {PROCESSED_DATA_FILE}')\nprint(f'  Thư mục lưu model: {MODEL_SAVE_DIR}')\nprint(f'  Thư mục lưu hình ảnh: {RESULT_IMAGE_DIR}')\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ffc93",
   "metadata": {},
   "source": [
    "## 4. Hàm Augmentation (SpecAugment)\n",
    "\n",
    "Định nghĩa các hàm `time_masking` và `frequency_masking` để thực hiện kỹ thuật SpecAugment trên Mel-spectrogram. Các hàm này sẽ tạo ra các mặt nạ thời gian và tần số ngẫu nhiên để tăng cường tính đa dạng của dữ liệu huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27172473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_masking(spectrogram, T=20, num_masks=2, replace_with_zero=True, axis_to_mask=1):\n    cloned = np.copy(spectrogram)\n    len_frames = cloned.shape[axis_to_mask]\n\n    for _ in range(num_masks):\n        t = random.randint(0, T)\n        if len_frames == 0: continue\n        t0 = random.randint(0, len_frames - t) if len_frames > t else 0\n\n        if replace_with_zero:\n            if cloned.ndim == 3:\n                cloned[:, t0:t0 + t, :] = 0\n            else:\n                cloned[:, t0:t0 + t] = 0\n        else:\n            mean_val = np.mean(cloned)\n            if cloned.ndim == 3:\n                cloned[:, t0:t0 + t, :] = mean_val\n            else:\n                cloned[:, t0:t0 + t] = mean_val\n    return cloned\n\ndef frequency_masking(spectrogram, F=15, num_masks=2, replace_with_zero=True, axis_to_mask=0):\n    cloned = np.copy(spectrogram)\n    num_mels = cloned.shape[axis_to_mask]\n\n    for _ in range(num_masks):\n        f = random.randint(0, F)\n        if num_mels == 0: continue\n        f0 = random.randint(0, num_mels - f) if num_mels > f else 0\n\n        if replace_with_zero:\n            if cloned.ndim == 3:\n                cloned[f0:f0 + f, :, :] = 0\n            else:\n                cloned[f0:f0 + f] = 0\n        else:\n            mean_val = np.mean(cloned)\n            if cloned.ndim == 3:\n                cloned[f0:f0 + f, :, :] = mean_val\n            else:\n                cloned[f0:f0 + f] = mean_val\n    return cloned\n\nprint(\"Các hàm SpecAugment đã được định nghĩa.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a08ca",
   "metadata": {},
   "source": [
    "## 5. Keras Sequence cho Data Augmentation\n",
    "\n",
    "Class `SpecAugmentSequence` kế thừa từ `tf.keras.utils.Sequence` để cung cấp dữ liệu theo từng batch cho Keras, đồng thời áp dụng SpecAugment (time masking và frequency masking) trực tiếp trong quá trình huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceeb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecAugmentSequence(Sequence):\n    def __init__(self, x_set, y_set, batch_size,\n                 apply_time_mask=True, time_mask_param_T=20, num_time_masks=2,\n                 apply_freq_mask=True, freq_mask_param_F=15, num_freq_masks=2,\n                 augment_prob=0.6):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.apply_time_mask = apply_time_mask\n        self.time_mask_param_T = time_mask_param_T\n        self.num_time_masks = num_time_masks\n        self.apply_freq_mask = apply_freq_mask\n        self.freq_mask_param_F = freq_mask_param_F\n        self.num_freq_masks = num_freq_masks\n        self.augment_prob = augment_prob\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = self.x[batch_x_indices]\n        batch_y = self.y[batch_x_indices]\n\n        augmented_batch_x = []\n        for spec_idx in range(batch_x.shape[0]):\n            spec = batch_x[spec_idx]\n            \n            augmented_spec = np.copy(spec)\n            if random.random() < self.augment_prob:\n                if self.apply_time_mask:\n                    augmented_spec = time_masking(augmented_spec, T=self.time_mask_param_T, num_masks=self.num_time_masks, axis_to_mask=1)\n                if self.apply_freq_mask:\n                    augmented_spec = frequency_masking(augmented_spec, F=self.freq_mask_param_F, num_masks=self.num_freq_masks, axis_to_mask=0)\n            augmented_batch_x.append(augmented_spec)\n\n        return np.array(augmented_batch_x), batch_y\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.x))\n        np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01882a23",
   "metadata": {},
   "source": [
    "## 6. Hàm tải và tiền xử lý dữ liệu\n",
    "\n",
    "Các hàm này chịu trách nhiệm đọc file âm thanh, chuyển đổi chúng thành Mel-spectrogram, chuẩn hóa kích thước và gán nhãn. Sau đó, dữ liệu sẽ được lưu vào file `.npz` để sử dụng lại mà không cần xử lý lại từ đầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd44d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(file_path, sr=SAMPLE_RATE, duration=FIXED_DURATION_SEC, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
    "    try:\n",
    "        samples, sr_orig = librosa.load(file_path, sr=sr, duration=duration, res_type='kaiser_fast')\n",
    "        if len(samples) < duration * sr:\n",
    "            samples = librosa.util.fix_length(samples, size=duration * sr)\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=samples, sr=sr_orig, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        if log_mel_spectrogram.shape[1] < EXPECTED_SPECTROGRAM_COLS:\n",
    "            pad_width = EXPECTED_SPECTROGRAM_COLS - log_mel_spectrogram.shape[1]\n",
    "            log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        elif log_mel_spectrogram.shape[1] > EXPECTED_SPECTROGRAM_COLS:\n",
    "            log_mel_spectrogram = log_mel_spectrogram[:, :EXPECTED_SPECTROGRAM_COLS]\n",
    "        return log_mel_spectrogram\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_dataset_from_paths(data_paths_labels):\n",
    "    features = []\n",
    "    labels = []\n",
    "    total_files = len(data_paths_labels)\n",
    "    print(f\"Bắt đầu tạo dataset từ {total_files} file...\")\n",
    "    for i, (file_path, label) in enumerate(data_paths_labels):\n",
    "        spectrogram = load_and_preprocess_audio(file_path)\n",
    "        if spectrogram is not None:\n",
    "            features.append(spectrogram)\n",
    "            labels.append(label)\n",
    "        if (i + 1) % 200 == 0 or (i + 1) == total_files:\n",
    "            print(f\"  Đã xử lý {i+1}/{total_files} file.\")\n",
    "    if not features:\n",
    "        print(\"Không có features nào được tạo.\")\n",
    "        return np.array([]), np.array([])\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    if features.ndim == 3 and features.size > 0:\n",
    "        features = features.reshape(features.shape[0], IMG_HEIGHT, IMG_WIDTH, 1)\n",
    "    print(f\"Hoàn tất tạo dataset. Số lượng features: {features.shape}, Số lượng labels: {labels.shape}\")\n",
    "    return features, labels\n",
    "\n",
    "def reconstruct_file_lists(base_dir):\n",
    "    sets = ['train', 'val', 'test']\n",
    "    all_data = {'train': [], 'val': [], 'test': []}\n",
    "    for s in sets:\n",
    "        clean_dir = os.path.join(base_dir, 'clean', s)\n",
    "        stego_dir = os.path.join(base_dir, 'stego', s)\n",
    "\n",
    "        if os.path.exists(clean_dir):\n",
    "            for filename in os.listdir(clean_dir):\n",
    "                if filename.lower().endswith('.wav'):\n",
    "                    all_data[s].append((os.path.join(clean_dir, filename), 0))\n",
    "        else:\n",
    "            print(f\"CẢNH BÁO: Thư mục file sạch không tồn tại: {clean_dir}\")\n",
    "\n",
    "        if os.path.exists(stego_dir):\n",
    "            for filename in os.listdir(stego_dir):\n",
    "                if filename.lower().endswith('.wav'):\n",
    "                    all_data[s].append((os.path.join(stego_dir, filename), 1))\n",
    "        else:\n",
    "            print(f\"CẢNH BÁO: Thư mục file stego không tồn tại: {stego_dir}\")\n",
    "\n",
    "        random.shuffle(all_data[s])\n",
    "\n",
    "    print(f\"Đã tái tạo danh sách file: Train ({len(all_data['train'])}), Val ({len(all_data['val'])}), Test ({len(all_data['test'])}).\")\n",
    "    return all_data['train'], all_data['val'], all_data['test']\n",
    "\n",
    "def prepare_and_save_data(data_parent_dir, output_filepath):\n",
    "    print(\"--- Bước 1 & 2: Tải, Tiền xử lý Dữ liệu và Lưu trữ ---\")\n",
    "    final_train_data, final_val_data, final_test_data = reconstruct_file_lists(data_parent_dir)\n",
    "\n",
    "    if not final_train_data:\n",
    "        print(\"LỖI: Không có dữ liệu huấn luyện nào được tìm thấy.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    X_train, y_train = create_dataset_from_paths(final_train_data)\n",
    "\n",
    "    X_val, y_val = np.array([]), np.array([])\n",
    "    if final_val_data:\n",
    "        X_val, y_val = create_dataset_from_paths(final_val_data)\n",
    "\n",
    "    X_test, y_test = np.array([]), np.array([])\n",
    "    if final_test_data:\n",
    "        X_test, y_test = create_dataset_from_paths(final_test_data)\n",
    "\n",
    "    if X_train.size == 0:\n",
    "        print(\"LỖI: Không tạo được features cho tập huấn luyện.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    print(f\"--- Đang lưu dữ liệu đã xử lý vào '{output_filepath}' ---\")\n",
    "    data_to_save = {'X_train': X_train, 'y_train': y_train}\n",
    "    if X_val.size > 0:\n",
    "        data_to_save['X_val'] = X_val\n",
    "        data_to_save['y_val'] = y_val\n",
    "    if X_test.size > 0:\n",
    "        data_to_save['X_test'] = X_test\n",
    "        data_to_save['y_test'] = y_test\n",
    "\n",
    "    np.savez_compressed(output_filepath, **data_to_save)\n",
    "    print(\"--- Lưu dữ liệu thành công ---\")\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "print(\"Các hàm tiền xử lý dữ liệu đã được định nghĩa.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc4789",
   "metadata": {},
   "source": [
    "## 7. Hàm xây dựng mô hình CNN\n",
    "\n",
    "Định nghĩa kiến trúc của mô hình CNN. Mô hình bao gồm các lớp tích chập (Conv2D), BatchNormalization, MaxPooling, Dropout và các lớp Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfaecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape):\n    model = Sequential()\n    l2_rate = 0.0015\n\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same',\n                     kernel_regularizer=regularizers.l2(l2_rate)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(l2_rate)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.35))\n\n    model.add(Conv2D(96, (3, 3), activation='relu', padding='same',\n                      kernel_regularizer=regularizers.l2(l2_rate)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.4))\n\n    model.add(Flatten())\n\n    model.add(Dense(128, activation='relu',\n                    kernel_regularizer=regularizers.l2(l2_rate)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.55))\n\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.summary()\n    return model\n\nprint(\"Hàm build_cnn_model đã được định nghĩa.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cb488",
   "metadata": {},
   "source": [
    "## 8. Chuẩn bị thư mục và tải/xử lý dữ liệu\n",
    "\n",
    "Ở bước này, chúng ta sẽ kiểm tra và tạo các thư mục cần thiết, sau đó cố gắng tải dữ liệu đã tiền xử lý từ file `.npz`. Nếu file không tồn tại hoặc bị lỗi, chương trình sẽ tự động chạy lại quá trình tiền xử lý từ các file `.wav` gốc và lưu lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f627415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục nếu chưa tồn tại\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(DATA_PARENT_DIR):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Không tìm thấy DATA_PARENT_DIR: {DATA_PARENT_DIR}. Hãy kiểm tra lại đường dẫn dataset trên Kaggle.\"\n",
    "    )\n",
    "\n",
    "print(f\"Notebook sẽ sử dụng file '{PROCESSED_DATA_FILE}' tại thư mục làm việc: {WORKING_DIR}\\n\")\n",
    "\n",
    "processed_data_file_path = os.path.join(WORKING_DIR, PROCESSED_DATA_FILE)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = None, None, None, None, None, None\n",
    "\n",
    "if os.path.exists(processed_data_file_path):\n",
    "    print(f\"--- Đang tải dữ liệu đã xử lý từ '{processed_data_file_path}' ---\")\n",
    "    try:\n",
    "        data = np.load(processed_data_file_path)\n",
    "        X_train = data['X_train']\n",
    "        y_train = data['y_train']\n",
    "        X_val = data.get('X_val', np.array([]))\n",
    "        y_val = data.get('y_val', np.array([]))\n",
    "        X_test = data.get('X_test', np.array([]))\n",
    "        y_test = data.get('y_test', np.array([]))\n",
    "        print('--- Tải dữ liệu thành công ---')\n",
    "        if X_train.size == 0:\n",
    "            raise ValueError('Dữ liệu huấn luyện rỗng.')\n",
    "    except Exception as exc:\n",
    "        print(f\"Lỗi khi tải dữ liệu đã xử lý: {exc}. Sẽ tiến hành tạo lại dữ liệu.\\n\")\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = prepare_and_save_data(\n",
    "            DATA_PARENT_DIR,\n",
    "            processed_data_file_path\n",
    "        )\n",
    "else:\n",
    "    print(f\"--- Không tìm thấy dữ liệu đã xử lý. Đang tạo mới tại '{processed_data_file_path}' ---\\n\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = prepare_and_save_data(\n",
    "        DATA_PARENT_DIR,\n",
    "        processed_data_file_path\n",
    "    )\n",
    "\n",
    "if X_train is None or X_train.size == 0:\n",
    "    raise RuntimeError('Không thể chuẩn bị dữ liệu huấn luyện. Dừng notebook.')\n",
    "\n",
    "\n",
    "def ensure_channel_dimension(arr, target_channels=None):\n",
    "    if arr is None or arr.size == 0:\n",
    "        return arr\n",
    "    if arr.ndim == 3:\n",
    "        arr = np.expand_dims(arr, axis=-1)\n",
    "    elif arr.ndim != 4:\n",
    "        raise ValueError(f'Dữ liệu đầu vào phải có 3 hoặc 4 chiều, nhận được shape {arr.shape}')\n",
    "    if target_channels is not None and arr.shape[-1] != target_channels:\n",
    "        if arr.shape[-1] == 1 and target_channels == 3:\n",
    "            arr = np.repeat(arr, target_channels, axis=-1)\n",
    "        elif arr.shape[-1] == 3 and target_channels == 1:\n",
    "            arr = arr.mean(axis=-1, keepdims=True, dtype=arr.dtype)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Không thể chuyển số kênh từ {arr.shape[-1]} sang {target_channels} với dữ liệu hiện có.'\n",
    "            )\n",
    "    return arr\n",
    "\n",
    "\n",
    "X_train = ensure_channel_dimension(X_train)\n",
    "if X_val.size > 0:\n",
    "    X_val = ensure_channel_dimension(X_val, target_channels=X_train.shape[-1])\n",
    "if X_test.size > 0:\n",
    "    X_test = ensure_channel_dimension(X_test, target_channels=X_train.shape[-1])\n",
    "\n",
    "INPUT_SHAPE = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "\n",
    "print(f\"Kích thước X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Số kênh đầu vào sau chuẩn hóa: {INPUT_SHAPE[-1]}\")\n",
    "if X_val.size > 0:\n",
    "    print(f\"Kích thước X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "else:\n",
    "    print('Không có tập validation.')\n",
    "if X_test.size > 0:\n",
    "    print(f\"Kích thước X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "else:\n",
    "    print('Không có tập test.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32526c05",
   "metadata": {},
   "source": [
    "## 9. Tạo Data Generator và Compile Model\n",
    "\n",
    "Khởi tạo `SpecAugmentSequence` để cung cấp dữ liệu huấn luyện với augmentation. Sau đó, xây dựng mô hình CNN và compile nó với optimizer Adam, hàm lỗi `binary_crossentropy` và các metrics cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- THAM SỐ CHO SPEC AUGMENT ---\n",
    "TIME_MASK_PARAM_T = 20\n",
    "NUM_TIME_MASKS = 2\n",
    "FREQ_MASK_PARAM_F = 15\n",
    "NUM_FREQ_MASKS = 2\n",
    "AUGMENT_PROB = 0.6\n",
    "\n",
    "train_data_generator = SpecAugmentSequence(\n",
    "    X_train, y_train, BATCH_SIZE,\n",
    "    apply_time_mask=True, time_mask_param_T=TIME_MASK_PARAM_T, num_time_masks=NUM_TIME_MASKS,\n",
    "    apply_freq_mask=True, freq_mask_param_F=FREQ_MASK_PARAM_F, num_freq_masks=NUM_FREQ_MASKS,\n",
    "    augment_prob=AUGMENT_PROB\n",
    ")\n",
    "print(\"Đã tạo Data Generator với SpecAugment cho tập huấn luyện.\")\n",
    "print(f\"  Time Masking: T={TIME_MASK_PARAM_T}, num_masks={NUM_TIME_MASKS}\")\n",
    "print(f\"  Frequency Masking: F={FREQ_MASK_PARAM_F}, num_masks={NUM_FREQ_MASKS}\")\n",
    "print(f\"  Augmentation Probability: {AUGMENT_PROB*100:.1f}%\")\n",
    "\n",
    "print(\"--- Xây dựng Mô hình CNN ---\")\n",
    "\n",
    "model = build_cnn_model(INPUT_SHAPE)\n",
    "\n",
    "# Compile model với Learning Rate Schedule\n",
    "if X_train.size > 0:\n",
    "    steps_per_epoch = max(len(X_train) // BATCH_SIZE, 1)\n",
    "    decay_steps_actual = steps_per_epoch * 10\n",
    "    decay_rate_actual = 0.90\n",
    "\n",
    "    initial_learning_rate = 1e-4\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps_actual,\n",
    "        decay_rate=decay_rate_actual,\n",
    "        staircase=True\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    print(f\"Sử dụng ExponentialDecay LR Schedule với initial_lr={initial_learning_rate}, decay_steps={decay_steps_actual}, decay_rate={decay_rate_actual}\")\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    print(\"CẢNH BÁO: X_train rỗng, sử dụng Adam LR mặc định (1e-4).\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
    ")\n",
    "print(\"Đã compile model với LR schedule và các metrics.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10b982",
   "metadata": {},
   "source": [
    "## 10. Huấn luyện Mô hình\n",
    "\n",
    "Tiến hành huấn luyện mô hình sử dụng Data Generator. Các callbacks `EarlyStopping` và `ModelCheckpoint` được sử dụng để dừng huấn luyện sớm khi validation loss không cải thiện và lưu lại trọng số của mô hình tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Bắt đầu Huấn luyện Mô hình ---\")\n",
    "\n",
    "model_id = 'audio_cnn'\n",
    "model_checkpoint_filename = f'best_model_{model_id}.keras'\n",
    "training_history_plot_filename = f'training_history_{model_id}.png'\n",
    "\n",
    "model_checkpoint_path = os.path.join(MODEL_SAVE_DIR, model_checkpoint_filename)\n",
    "training_history_plot_path = os.path.join(RESULT_IMAGE_DIR, training_history_plot_filename)\n",
    "\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(\n",
    "        model_checkpoint_path,\n",
    "        monitor='val_loss' if X_val.size > 0 else 'loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "if X_val.size > 0:\n",
    "    callbacks_list.insert(0, EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ))\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val) if X_val.size > 0 else None,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "print('Huấn luyện mô hình hoàn tất.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fa62e",
   "metadata": {},
   "source": [
    "## 11. Đánh giá và Tìm ngưỡng tối ưu trên tập Validation\n",
    "\n",
    "Sau khi huấn luyện, chúng ta sẽ tải lại mô hình tốt nhất (từ checkpoint) và sử dụng tập validation để tìm ra ngưỡng phân loại tối ưu (dựa trên F1-score). Ngưỡng này sau đó sẽ được áp dụng cho tập kiểm thử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f76f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "best_model_to_evaluate = None\n",
    "if os.path.exists(model_checkpoint_path):\n",
    "    print(f\"\\n--- Tải lại model tốt nhất từ '{model_checkpoint_path}' để đánh giá ---\")\n",
    "    try:\n",
    "        best_model_to_evaluate = tf.keras.models.load_model(model_checkpoint_path)\n",
    "        print(\"Đã load lại model tốt nhất từ checkpoint.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Không thể tải lại model tốt nhất từ checkpoint: {e}.\")\n",
    "        if 'model' in locals() and model is not None:\n",
    "            print(\"Sử dụng model hiện tại sau khi fit (nếu có) để đánh giá.\")\n",
    "            best_model_to_evaluate = model\n",
    "else:\n",
    "    print(\"CẢNH BÁO: Không tìm thấy file checkpoint. Sử dụng model hiện tại sau khi fit để đánh giá.\")\n",
    "    best_model_to_evaluate = model\n",
    "\n",
    "best_threshold_f1 = 0.5\n",
    "if best_model_to_evaluate is not None and X_val.size > 0 and y_val.size > 0:\n",
    "    print(\"\\n--- Tìm Ngưỡng Quyết Định Tối Ưu trên Tập Validation ---\")\n",
    "    y_pred_proba_val = best_model_to_evaluate.predict(X_val)\n",
    "\n",
    "    precisions_pr, recalls_pr, thresholds_pr = precision_recall_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "    f1_scores_val_pr = np.array([])\n",
    "    if len(precisions_pr) > 1 and len(recalls_pr) > 1:\n",
    "        f1_scores_val_pr = (2 * precisions_pr[:-1] * recalls_pr[:-1]) / (precisions_pr[:-1] + recalls_pr[:-1] + 1e-9)\n",
    "\n",
    "    if f1_scores_val_pr.size > 0:\n",
    "        best_f1_idx_val = np.argmax(f1_scores_val_pr)\n",
    "        if best_f1_idx_val < len(thresholds_pr):\n",
    "            best_threshold_f1 = thresholds_pr[best_f1_idx_val]\n",
    "            print(f\"  Ngưỡng tối ưu cho F1-score trên tập Validation: {best_threshold_f1:.4f}\")\n",
    "            print(f\"  F1-score cao nhất tương ứng: {f1_scores_val_pr[best_f1_idx_val]:.4f}\")\n",
    "            print(f\"  Precision tại ngưỡng này: {precisions_pr[best_f1_idx_val]:.4f}\")\n",
    "            print(f\"  Recall tại ngưỡng này: {recalls_pr[best_f1_idx_val]:.4f}\")\n",
    "        else:\n",
    "            print(\"  Lỗi khi xác định ngưỡng F1 tốt nhất (index out of bounds for thresholds_pr). Sử dụng ngưỡng mặc định 0.5.\")\n",
    "            best_threshold_f1 = 0.5\n",
    "    else:\n",
    "        print(\"  Không thể tính F1-scores trên tập Validation (mảng F1 rỗng hoặc PR curve không đủ điểm). Sử dụng ngưỡng mặc định 0.5.\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recalls_pr, precisions_pr, marker='.', label='Precision-Recall Curve')\n",
    "    if 'best_f1_idx_val' in locals() and best_f1_idx_val < min(len(recalls_pr), len(precisions_pr)) and f1_scores_val_pr.size > 0:\n",
    "        label_text = f\"Best F1 Val (Thresh={best_threshold_f1:.2f}, F1={f1_scores_val_pr[best_f1_idx_val]:.2f})\"\n",
    "        plt.scatter(recalls_pr[best_f1_idx_val], precisions_pr[best_f1_idx_val], marker='o', s=100, color='red', label=label_text)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve on Validation Set')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    pr_curve_path = os.path.join(RESULT_IMAGE_DIR, f\"precision_recall_curve_val_{os.path.splitext(model_checkpoint_filename)[0]}.png\")\n",
    "    try:\n",
    "        plt.savefig(pr_curve_path)\n",
    "        print(f\"  Đã lưu Precision-Recall curve vào: {pr_curve_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e_plot:\n",
    "        print(f\"  Lỗi khi lưu Precision-Recall curve: {e_plot}\")\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"Không có dữ liệu validation để tìm ngưỡng tối ưu, hoặc không load được model. Sử dụng ngưỡng mặc định 0.5.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee8663",
   "metadata": {},
   "source": [
    "## 12. Đánh giá trên tập Test và hiển thị kết quả\n",
    "\n",
    "Cuối cùng, mô hình sẽ được đánh giá trên tập kiểm thử (test set) bằng cách sử dụng ngưỡng tối ưu đã tìm được. Các báo cáo đánh giá như `classification_report`, `confusion_matrix` và đồ thị lịch sử huấn luyện sẽ được hiển thị và lưu lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "if best_model_to_evaluate is not None and X_test.size > 0 and y_test.size > 0:\n",
    "    print(\"\\n--- Đánh giá Mô hình trên Tập Kiểm thử (với ngưỡng đã chọn) ---\")\n",
    "    y_pred_proba_test = best_model_to_evaluate.predict(X_test)\n",
    "    y_pred_test_custom_threshold = (y_pred_proba_test >= best_threshold_f1).astype(int)\n",
    "\n",
    "    print(f\"\\nKết quả đánh giá trên tập kiểm thử với ngưỡng: {best_threshold_f1:.4f}\")\n",
    "    target_names = ['Lớp Sạch (0)', 'Lớp Stego (1)']\n",
    "    try:\n",
    "        print(classification_report(y_test, y_pred_test_custom_threshold, target_names=target_names, digits=4, zero_division=0))\n",
    "    except ValueError as ve:\n",
    "        print(f\"Lỗi khi tạo classification report: {ve}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_test_custom_threshold)\n",
    "    print(\"Confusion Matrix trên tập Test:\")\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nKết quả đánh giá từ model.evaluate (ngưỡng mặc định 0.5 của Keras):\")\n",
    "    eval_results = best_model_to_evaluate.evaluate(X_test, y_test, verbose=0)\n",
    "    metric_names = best_model_to_evaluate.metrics_names\n",
    "    for name, value in zip(metric_names, eval_results):\n",
    "        print(f\"  {name.capitalize()}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"\\nKhông có dữ liệu kiểm thử để đánh giá hoặc không có model.\")\n",
    "\n",
    "try:\n",
    "    if hasattr(history, 'history') and history.history:\n",
    "        metrics_to_plot = ['accuracy', 'loss', 'precision', 'recall', 'auc']\n",
    "        available_metrics = [m for m in metrics_to_plot if m in history.history or f'val_{m}' in history.history]\n",
    "\n",
    "        if available_metrics:\n",
    "            num_rows_plot = (len(available_metrics) + 1) // 2\n",
    "            plt.figure(figsize=(15, 5 * num_rows_plot))\n",
    "            plot_idx = 1\n",
    "            for metric in available_metrics:\n",
    "                has_train_metric = metric in history.history and history.history[metric]\n",
    "                has_val_metric = f'val_{metric}' in history.history and history.history[f'val_{metric}']\n",
    "\n",
    "                if has_train_metric or has_val_metric:\n",
    "                    plt.subplot(num_rows_plot, 2, plot_idx)\n",
    "                    if has_train_metric:\n",
    "                        plt.plot(history.history[metric], label=f'Training {metric.capitalize()}')\n",
    "                    if has_val_metric:\n",
    "                        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}')\n",
    "\n",
    "                    plt.title(metric.capitalize())\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.ylabel(metric.capitalize())\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    plot_idx += 1\n",
    "\n",
    "            plt.tight_layout()\n",
    "            try:\n",
    "                plt.savefig(training_history_plot_path)\n",
    "                print(f\"Đã lưu đồ thị lịch sử huấn luyện vào '{training_history_plot_path}'\")\n",
    "                plt.close()\n",
    "            except Exception as e_plot_hist:\n",
    "                print(f\"Lỗi khi lưu đồ thị lịch sử huấn luyện: {e_plot_hist}\")\n",
    "                plt.close()\n",
    "        else:\n",
    "            print(\"Không có metric nào trong history để vẽ đồ thị.\")\n",
    "    else:\n",
    "        print(\"History object không có dữ liệu để vẽ đồ thị.\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi vẽ đồ thị lịch sử huấn luyện: {e}\")\n",
    "\n",
    "print(\"Hoàn tất quá trình huấn luyện và đánh giá.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}