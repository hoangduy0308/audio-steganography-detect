{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d2572dd0-2588-4cf5-99f1-da469e695324",
    "_uuid": "25f99c72-1265-46f9-813e-6ae35ff801d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:23.564948Z",
     "iopub.status.busy": "2025-10-29T17:25:23.564690Z",
     "iopub.status.idle": "2025-10-29T17:25:28.629529Z",
     "shell.execute_reply": "2025-10-29T17:25:28.628813Z",
     "shell.execute_reply.started": "2025-10-29T17:25:23.564930Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting resampy\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from resampy) (1.26.4)\n",
      "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.11/dist-packages (from resampy) (0.60.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.53->resampy) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->resampy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->resampy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->resampy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->resampy) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->resampy) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->resampy) (2.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->resampy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->resampy) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->resampy) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->resampy) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->resampy) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: resampy\n",
      "Successfully installed resampy-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install resampy pandas scikit-learn matplotlib librosa tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:28.630694Z",
     "iopub.status.busy": "2025-10-29T17:25:28.630480Z",
     "iopub.status.idle": "2025-10-29T17:25:43.024726Z",
     "shell.execute_reply": "2025-10-29T17:25:43.024120Z",
     "shell.execute_reply.started": "2025-10-29T17:25:28.630673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 17:25:30.298888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761758730.536537      79 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761758730.599817      79 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, BinaryAccuracy\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc as sklearn_auc, f1_score, confusion_matrix, classification_report\n",
    "import random\n",
    "import wave\n",
    "import shutil\n",
    "import pandas as pd\n",
    "GLOBAL_SEED = 2025\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:43.026066Z",
     "iopub.status.busy": "2025-10-29T17:25:43.025596Z",
     "iopub.status.idle": "2025-10-29T17:25:43.956172Z",
     "shell.execute_reply": "2025-10-29T17:25:43.955338Z",
     "shell.execute_reply.started": "2025-10-29T17:25:43.026047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761758743.950402      79 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1761758743.951120      79 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# --- Cấu hình GPU ---\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        if len(gpus) > 1:\n",
    "            tf.config.set_visible_devices([gpus[0]], 'GPU')\n",
    "            print(f'Dang gioi han su dung 1 GPU: {gpus[0].name}')\n",
    "        visible_gpus = tf.config.list_physical_devices('GPU')\n",
    "        for gpu in visible_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(visible_gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:43.958240Z",
     "iopub.status.busy": "2025-10-29T17:25:43.957976Z",
     "iopub.status.idle": "2025-10-29T17:25:43.970498Z",
     "shell.execute_reply": "2025-10-29T17:25:43.969952Z",
     "shell.execute_reply.started": "2025-10-29T17:25:43.958221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các đường dẫn và tham số đã được cấu hình.\n",
      "Dữ liệu xử lý sẽ được lưu/tải tại: /kaggle/working/audio_data.npz\n",
      "Model checkpoints sẽ được lưu tại: /kaggle/working/model_output\n",
      "Hình ảnh kết quả sẽ được lưu tại: /kaggle/working/result_images\n"
     ]
    }
   ],
   "source": [
    "# --- CAU HINH CHUNG ---\n",
    "DATA_PARENT_DIR = '/kaggle/input/data-lsb/data' # Đường dẫn dataset Kaggle (Cần thay đổi nếu chạy local)\n",
    "PROCESSED_DATA_FILE = 'audio_data.npz' # Tên file data đã xử lý (Lưu ở thư mục hiện tại)\n",
    "\n",
    "# --- Tham số Audio ---\n",
    "SAMPLE_RATE = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 128\n",
    "FIXED_DURATION_SEC = 4\n",
    "EXPECTED_SPECTROGRAM_COLS = int(np.ceil(FIXED_DURATION_SEC * SAMPLE_RATE / HOP_LENGTH))\n",
    "\n",
    "# --- Tham số Model ---\n",
    "IMG_WIDTH, IMG_HEIGHT = EXPECTED_SPECTROGRAM_COLS, N_MELS\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "EPOCHS = 180\n",
    "BATCH_SIZE = 16  # Batch size co so cho moi GPU (GLOBAL_BATCH_SIZE = BATCH_SIZE * so replica)\n",
    "\n",
    "# --- Đường dẫn Output ---\n",
    "WORKING_DIR = '/kaggle/working/' if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ else '.'\n",
    "MODEL_SAVE_DIR = os.path.join(WORKING_DIR, 'model_output')\n",
    "RESULT_IMAGE_DIR = os.path.join(WORKING_DIR, 'result_images')\n",
    "PROCESSED_DATA_FULL_PATH = os.path.join(WORKING_DIR, PROCESSED_DATA_FILE)\n",
    "\n",
    "# --- Tên file Output ---\n",
    "run_id = \"audio_steganalysis_v1\"\n",
    "model_checkpoint_filename = f'best_model_{run_id}.keras'\n",
    "training_history_plot_filename = f'training_history_{run_id}.png'\n",
    "pr_curve_filename = f'precision_recall_curve_val_{run_id}.png'\n",
    "\n",
    "# Tạo thư mục output\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_IMAGE_DIR, exist_ok=True)\n",
    "print(\"Các đường dẫn và tham số đã được cấu hình.\")\n",
    "print(f\"Dữ liệu xử lý sẽ được lưu/tải tại: {PROCESSED_DATA_FULL_PATH}\")\n",
    "print(f\"Model checkpoints sẽ được lưu tại: {MODEL_SAVE_DIR}\")\n",
    "print(f\"Hình ảnh kết quả sẽ được lưu tại: {RESULT_IMAGE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:43.971541Z",
     "iopub.status.busy": "2025-10-29T17:25:43.971293Z",
     "iopub.status.idle": "2025-10-29T17:25:43.992462Z",
     "shell.execute_reply": "2025-10-29T17:25:43.991782Z",
     "shell.execute_reply.started": "2025-10-29T17:25:43.971520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã định nghĩa các hàm augmentation (SpecAugment + noise/gain).\n"
     ]
    }
   ],
   "source": [
    "# --- HÀM AUGMENTATION ---\n",
    "def time_masking(spectrogram, T=20, num_masks=2, replace_with_zero=True, axis_to_mask=1):\n",
    "    cloned = np.copy(spectrogram)\n",
    "    len_frames = cloned.shape[axis_to_mask]\n",
    "    for _ in range(num_masks):\n",
    "        t = random.randint(0, T)\n",
    "        if len_frames == 0:\n",
    "            continue\n",
    "        t0 = random.randint(0, len_frames - t) if len_frames > t else 0\n",
    "        if replace_with_zero:\n",
    "            if cloned.ndim == 3:\n",
    "                cloned[:, t0:t0 + t, :] = 0\n",
    "            else:\n",
    "                cloned[:, t0:t0 + t] = 0\n",
    "        else:\n",
    "            mean_val = np.mean(cloned)\n",
    "            if cloned.ndim == 3:\n",
    "                cloned[:, t0:t0 + t, :] = mean_val\n",
    "            else:\n",
    "                cloned[:, t0:t0 + t] = mean_val\n",
    "    return cloned\n",
    "\n",
    "\n",
    "def frequency_masking(spectrogram, F=15, num_masks=2, replace_with_zero=True, axis_to_mask=0):\n",
    "    cloned = np.copy(spectrogram)\n",
    "    num_mels = cloned.shape[axis_to_mask]\n",
    "    for _ in range(num_masks):\n",
    "        f = random.randint(0, F)\n",
    "        if num_mels == 0:\n",
    "            continue\n",
    "        f0 = random.randint(0, num_mels - f) if num_mels > f else 0\n",
    "        if replace_with_zero:\n",
    "            if cloned.ndim == 3:\n",
    "                cloned[f0:f0 + f, :, :] = 0\n",
    "            else:\n",
    "                cloned[f0:f0 + f, :] = 0\n",
    "        else:\n",
    "            mean_val = np.mean(cloned)\n",
    "            if cloned.ndim == 3:\n",
    "                cloned[f0:f0 + f, :, :] = mean_val\n",
    "            else:\n",
    "                cloned[f0:f0 + f, :] = mean_val\n",
    "    return cloned\n",
    "\n",
    "\n",
    "def additive_gaussian_noise(spectrogram, std=0.01):\n",
    "    noise = np.random.normal(0.0, std, spectrogram.shape)\n",
    "    return spectrogram + noise\n",
    "\n",
    "\n",
    "def random_gain(spectrogram, min_gain_db=-2.0, max_gain_db=2.0):\n",
    "    gain_db = np.random.uniform(min_gain_db, max_gain_db)\n",
    "    gain = 10.0 ** (gain_db / 20.0)\n",
    "    return spectrogram * gain\n",
    "\n",
    "\n",
    "print(\"Đã định nghĩa các hàm augmentation (SpecAugment + noise/gain).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:43.993866Z",
     "iopub.status.busy": "2025-10-29T17:25:43.993267Z",
     "iopub.status.idle": "2025-10-29T17:25:44.010565Z",
     "shell.execute_reply": "2025-10-29T17:25:44.010006Z",
     "shell.execute_reply.started": "2025-10-29T17:25:43.993848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã định nghĩa class SpecAugmentSequence (aug mở rộng).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Keras Sequence (SpecAugment + Noise/Gain) ---\n",
    "class SpecAugmentSequence(Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_set,\n",
    "        y_set,\n",
    "        batch_size,\n",
    "        apply_time_mask=True,\n",
    "        time_mask_param_T=20,\n",
    "        num_time_masks=2,\n",
    "        apply_freq_mask=True,\n",
    "        freq_mask_param_F=15,\n",
    "        num_freq_masks=2,\n",
    "        apply_noise=True,\n",
    "        noise_std=0.01,\n",
    "        apply_gain=True,\n",
    "        gain_db_range=(-2.0, 2.0),\n",
    "        augment_prob=0.4,\n",
    "        shuffle=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.apply_time_mask = apply_time_mask\n",
    "        self.time_mask_param_T = time_mask_param_T\n",
    "        self.num_time_masks = num_time_masks\n",
    "        self.apply_freq_mask = apply_freq_mask\n",
    "        self.freq_mask_param_F = freq_mask_param_F\n",
    "        self.num_freq_masks = num_freq_masks\n",
    "        self.apply_noise = apply_noise\n",
    "        self.noise_std = noise_std\n",
    "        self.apply_gain = apply_gain\n",
    "        self.gain_db_range = gain_db_range\n",
    "        self.augment_prob = augment_prob\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x = self.x[batch_x_indices]\n",
    "        batch_y = self.y[batch_x_indices]\n",
    "\n",
    "        augmented_batch_x = []\n",
    "        for spec_idx in range(batch_x.shape[0]):\n",
    "            spec = batch_x[spec_idx]\n",
    "            augmented_spec = np.copy(spec)\n",
    "            if random.random() < self.augment_prob:\n",
    "                if self.apply_gain:\n",
    "                    augmented_spec = random_gain(\n",
    "                        augmented_spec,\n",
    "                        min_gain_db=self.gain_db_range[0],\n",
    "                        max_gain_db=self.gain_db_range[1]\n",
    "                    )\n",
    "                if self.apply_noise:\n",
    "                    augmented_spec = additive_gaussian_noise(augmented_spec, std=self.noise_std)\n",
    "                if self.apply_time_mask:\n",
    "                    augmented_spec = time_masking(\n",
    "                        augmented_spec,\n",
    "                        T=self.time_mask_param_T,\n",
    "                        num_masks=self.num_time_masks,\n",
    "                        axis_to_mask=1\n",
    "                    )\n",
    "                if self.apply_freq_mask:\n",
    "                    augmented_spec = frequency_masking(\n",
    "                        augmented_spec,\n",
    "                        F=self.freq_mask_param_F,\n",
    "                        num_masks=self.num_freq_masks,\n",
    "                        axis_to_mask=0\n",
    "                    )\n",
    "            augmented_batch_x.append(augmented_spec)\n",
    "\n",
    "        batch_x_to_return = np.asarray(augmented_batch_x, dtype=np.float32)\n",
    "        batch_x_to_return = np.nan_to_num(batch_x_to_return, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        np.clip(batch_x_to_return, -6.0, 6.0, out=batch_x_to_return)\n",
    "        batch_y = batch_y.astype(np.float32, copy=False)\n",
    "        batch_y = np.nan_to_num(batch_y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        if np.isnan(batch_x_to_return).any() or np.isinf(batch_x_to_return).any():\n",
    "            print(f\"*** LOI NGHIEM TRONG (Sequence): NaN/Inf tim thay TRUOC KHI RETURN batch_x index {idx} !!!\")\n",
    "        if np.isnan(batch_y).any() or np.isinf(batch_y).any():\n",
    "            print(f\"*** LOI NGHIEM TRONG (Sequence): NaN/Inf tim thay trong batch_y index {idx} !!!\")\n",
    "\n",
    "        return batch_x_to_return, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "print('Da dinh nghia class SpecAugmentSequence (aug mo rong).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:44.011342Z",
     "iopub.status.busy": "2025-10-29T17:25:44.011176Z",
     "iopub.status.idle": "2025-10-29T17:25:44.030020Z",
     "shell.execute_reply": "2025-10-29T17:25:44.029404Z",
     "shell.execute_reply.started": "2025-10-29T17:25:44.011329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã định nghĩa hàm build_cnn_model_with_bn (multi-channel).\n"
     ]
    }
   ],
   "source": [
    "# --- HAM XAY DUNG MO HINH CNN (Spectrogram 3 kenh) ---\n",
    "def build_cnn_model_with_bn(input_shape):\n",
    "    \"\"\"Xây dựng CNN nhiều tầng với BatchNorm + GlobalAveragePooling.\"\"\"\n",
    "    model = Sequential(name=\"audio_steganalysis_cnn_bn_multichannel\")\n",
    "    l2_rate = 5e-4\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape,\n",
    "                     kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.35))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256, kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "print(\"Đã định nghĩa hàm build_cnn_model_with_bn (multi-channel).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:44.031031Z",
     "iopub.status.busy": "2025-10-29T17:25:44.030734Z",
     "iopub.status.idle": "2025-10-29T17:25:44.049469Z",
     "shell.execute_reply": "2025-10-29T17:25:44.048785Z",
     "shell.execute_reply.started": "2025-10-29T17:25:44.031014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã định nghĩa các hàm xử lý dữ liệu (multi-channel spectrogram).\n"
     ]
    }
   ],
   "source": [
    "# --- Hàm tải và tiền xử lý dữ liệu ---\n",
    "def load_and_preprocess_audio(\n",
    "    file_path,\n",
    "    sr=SAMPLE_RATE,\n",
    "    duration=FIXED_DURATION_SEC,\n",
    "    n_mels=N_MELS,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    "):\n",
    "    try:\n",
    "        samples, _ = librosa.load(\n",
    "            file_path,\n",
    "            sr=sr,\n",
    "            duration=duration,\n",
    "            res_type='kaiser_fast'\n",
    "        )\n",
    "        target_length = int(duration * sr)\n",
    "        if len(samples) < target_length:\n",
    "            samples = librosa.util.fix_length(samples, size=target_length)\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=samples,\n",
    "            sr=sr,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels\n",
    "        )\n",
    "        mel_spectrogram = np.maximum(1e-10, mel_spectrogram)\n",
    "\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "        if log_mel_spectrogram.shape[1] < EXPECTED_SPECTROGRAM_COLS:\n",
    "            pad_width = EXPECTED_SPECTROGRAM_COLS - log_mel_spectrogram.shape[1]\n",
    "            log_mel_spectrogram = np.pad(\n",
    "                log_mel_spectrogram,\n",
    "                ((0, 0), (0, pad_width)),\n",
    "                mode='constant'\n",
    "            )\n",
    "        elif log_mel_spectrogram.shape[1] > EXPECTED_SPECTROGRAM_COLS:\n",
    "            log_mel_spectrogram = log_mel_spectrogram[:, :EXPECTED_SPECTROGRAM_COLS]\n",
    "\n",
    "        delta_1 = librosa.feature.delta(log_mel_spectrogram, order=1, mode='nearest')\n",
    "        delta_2 = librosa.feature.delta(log_mel_spectrogram, order=2, mode='nearest')\n",
    "\n",
    "        stacked = np.stack([log_mel_spectrogram, delta_1, delta_2], axis=-1)\n",
    "        stacked = np.nan_to_num(stacked, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return stacked\n",
    "    except Exception as e:\n",
    "        print(f\"Loi xu ly file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_dataset_from_paths(data_paths_labels):\n",
    "    features, labels = [], []\n",
    "    total_files = len(data_paths_labels)\n",
    "    if total_files == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    print(f\"Bat dau tao dataset tu {total_files} file...\")\n",
    "    for i, (file_path, label) in enumerate(data_paths_labels):\n",
    "        spectrogram = load_and_preprocess_audio(file_path)\n",
    "        if spectrogram is not None:\n",
    "            features.append(spectrogram)\n",
    "            labels.append(label)\n",
    "        if (i + 1) % 500 == 0 or (i + 1) == total_files:\n",
    "            print(f\"  Da xu ly {i+1}/{total_files} file.\")\n",
    "\n",
    "    if not features:\n",
    "        print(\"Khong co features nao duoc tao.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    if features.ndim == 3:\n",
    "        features = features[..., np.newaxis]\n",
    "    elif features.ndim != 4:\n",
    "        raise ValueError(f\"Hinh dang feature khong hop le: {features.shape}\")\n",
    "\n",
    "    print(f\"Hoan tat tao dataset. Features: {features.shape}, Labels: {labels.shape}\")\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def reconstruct_file_lists(base_dir):\n",
    "    sets = ['train', 'val', 'test']\n",
    "    all_data = {'train': [], 'val': [], 'test': []}\n",
    "    print('--- Tai tao danh sach file ---')\n",
    "    for s in sets:\n",
    "        clean_dir = os.path.join(base_dir, 'clean', s)\n",
    "        stego_dir = os.path.join(base_dir, 'stego', s)\n",
    "        clean_files = []\n",
    "        if os.path.exists(clean_dir):\n",
    "            clean_files = [\n",
    "                (os.path.join(clean_dir, fn), 0)\n",
    "                for fn in sorted(os.listdir(clean_dir))\n",
    "                if fn.lower().endswith('.wav')\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"CANH BAO: Khong tim thay thu muc sach: {clean_dir}\")\n",
    "        stego_files = []\n",
    "        if os.path.exists(stego_dir):\n",
    "            stego_files = [\n",
    "                (os.path.join(stego_dir, fn), 1)\n",
    "                for fn in sorted(os.listdir(stego_dir))\n",
    "                if fn.lower().endswith('.wav')\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"CANH BAO: Khong tim thay thu muc stego: {stego_dir}\")\n",
    "        all_data[s] = clean_files + stego_files\n",
    "        seed_offset = {'train': GLOBAL_SEED, 'val': GLOBAL_SEED + 1, 'test': GLOBAL_SEED + 2}[s]\n",
    "        rng = random.Random(seed_offset)\n",
    "        rng.shuffle(all_data[s])\n",
    "    print(f\"Train: {len(all_data['train'])}, Val: {len(all_data['val'])}, Test: {len(all_data['test'])} file.\")\n",
    "    return all_data['train'], all_data['val'], all_data['test']\n",
    "\n",
    "\n",
    "def prepare_and_save_data(data_parent_dir, output_filepath):\n",
    "    print(\"\\n--- Buoc 1 & 2: Tai, Tien xu ly Du lieu va Luu tru ---\")\n",
    "    final_train_data, final_val_data, final_test_data = reconstruct_file_lists(data_parent_dir)\n",
    "    if not final_train_data:\n",
    "        print(\"LOI: Khong co du lieu huan luyen.\")\n",
    "        return [np.array([])] * 6\n",
    "\n",
    "    X_train, y_train = create_dataset_from_paths(final_train_data)\n",
    "    X_val, y_val = create_dataset_from_paths(final_val_data)\n",
    "    X_test, y_test = create_dataset_from_paths(final_test_data)\n",
    "\n",
    "    if X_train.size == 0:\n",
    "        print(\"LOI: Khong tao duoc features cho tap huan luyen.\")\n",
    "        return [np.array([])] * 6\n",
    "\n",
    "    print(f\"\\n--- Dang luu du lieu da xu ly vao '{output_filepath}' ---\")\n",
    "    data_to_save = {'X_train': X_train, 'y_train': y_train}\n",
    "    if X_val.size > 0:\n",
    "        data_to_save.update({'X_val': X_val, 'y_val': y_val})\n",
    "    if X_test.size > 0:\n",
    "        data_to_save.update({'X_test': X_test, 'y_test': y_test})\n",
    "    np.savez_compressed(output_filepath, **data_to_save)\n",
    "    print('--- Luu du lieu thanh cong ---')\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "print(\"Đã định nghĩa các hàm xử lý dữ liệu (multi-channel spectrogram).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:44.050347Z",
     "iopub.status.busy": "2025-10-29T17:25:44.050151Z",
     "iopub.status.idle": "2025-10-29T17:25:44.066828Z",
     "shell.execute_reply": "2025-10-29T17:25:44.066070Z",
     "shell.execute_reply.started": "2025-10-29T17:25:44.050332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, BinaryAccuracy\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc as sklearn_auc, f1_score, confusion_matrix, classification_report\n",
    "import random\n",
    "import wave\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:25:44.067843Z",
     "iopub.status.busy": "2025-10-29T17:25:44.067628Z",
     "iopub.status.idle": "2025-10-29T17:41:20.891505Z",
     "shell.execute_reply": "2025-10-29T17:41:20.890610Z",
     "shell.execute_reply.started": "2025-10-29T17:25:44.067827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Khong tim thay du lieu da xu ly. Dang tao moi... ---\n",
      "\n",
      "--- Buoc 1 & 2: Tai, Tien xu ly Du lieu va Luu tru ---\n",
      "--- Tai tao danh sach file ---\n",
      "Train: 8130, Val: 1762, Test: 1710 file.\n",
      "Bat dau tao dataset tu 8130 file...\n",
      "  Da xu ly 500/8130 file.\n",
      "  Da xu ly 1000/8130 file.\n",
      "  Da xu ly 1500/8130 file.\n",
      "  Da xu ly 2000/8130 file.\n",
      "  Da xu ly 2500/8130 file.\n",
      "  Da xu ly 3000/8130 file.\n",
      "  Da xu ly 3500/8130 file.\n",
      "  Da xu ly 4000/8130 file.\n",
      "  Da xu ly 4500/8130 file.\n",
      "  Da xu ly 5000/8130 file.\n",
      "  Da xu ly 5500/8130 file.\n",
      "  Da xu ly 6000/8130 file.\n",
      "  Da xu ly 6500/8130 file.\n",
      "  Da xu ly 7000/8130 file.\n",
      "  Da xu ly 7500/8130 file.\n",
      "  Da xu ly 8000/8130 file.\n",
      "  Da xu ly 8130/8130 file.\n",
      "Hoan tat tao dataset. Features: (8130, 128, 173, 3), Labels: (8130,)\n",
      "Bat dau tao dataset tu 1762 file...\n",
      "  Da xu ly 500/1762 file.\n",
      "  Da xu ly 1000/1762 file.\n",
      "  Da xu ly 1500/1762 file.\n",
      "  Da xu ly 1762/1762 file.\n",
      "Hoan tat tao dataset. Features: (1762, 128, 173, 3), Labels: (1762,)\n",
      "Bat dau tao dataset tu 1710 file...\n",
      "  Da xu ly 500/1710 file.\n",
      "  Da xu ly 1000/1710 file.\n",
      "  Da xu ly 1500/1710 file.\n",
      "  Da xu ly 1710/1710 file.\n",
      "Hoan tat tao dataset. Features: (1710, 128, 173, 3), Labels: (1710,)\n",
      "\n",
      "--- Dang luu du lieu da xu ly vao '/kaggle/working/audio_data.npz' ---\n",
      "--- Luu du lieu thanh cong ---\n",
      "\n",
      "Kich thuoc du lieu TRUOC xu ly: Train=(8130, 128, 173, 3), Val=(1762, 128, 173, 3), Test=(1710, 128, 173, 3)\n",
      "Phan bo y_train: {0: 4065, 1: 4065}\n",
      "Phan bo y_val: {0: 881, 1: 881}\n",
      "Phan bo y_test: {0: 855, 1: 855}\n",
      "\n",
      "--- CHUAN HOA DU LIEU ---\n",
      "Train mean (per channel): -11.9284, -0.0281, -0.0039\n",
      "Train std (per channel): 27.6300, 0.8936, 0.4666\n",
      "Da ap dung chuan hoa.\n",
      "\n",
      "--- KIEM TRA MIN/MAX (SAU CHUAN HOA) ---\n",
      "X_train Min/Max: -16.2845 / 16.3728\n",
      "X_val Min/Max: -16.3286 / 15.4910\n",
      "\n",
      "--- EP KIEU DU LIEU SANG float32 ---\n",
      "Da ep kieu sang float32.\n"
     ]
    }
   ],
   "source": [
    "# --- BUOC CHUAN BI DU LIEU (TAI HOAC TAO MOI) ---\n",
    "\n",
    "# Khởi tạo các biến dữ liệu\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = (np.array([]),) * 6\n",
    "\n",
    "# --- Tải hoặc Tạo Dữ liệu ---\n",
    "expected_channels = INPUT_SHAPE[-1]\n",
    "if os.path.exists(PROCESSED_DATA_FULL_PATH):\n",
    "    print(f\"--- Dang tai du lieu da xu ly tu '{PROCESSED_DATA_FULL_PATH}' ---\")\n",
    "    try:\n",
    "        data = np.load(PROCESSED_DATA_FULL_PATH)\n",
    "        X_train = data['X_train']\n",
    "        y_train = data['y_train']\n",
    "        X_val = data.get('X_val', np.array([]))\n",
    "        y_val = data.get('y_val', np.array([]))\n",
    "        X_test = data.get('X_test', np.array([]))\n",
    "        y_test = data.get('y_test', np.array([]))\n",
    "        print('--- Tai du lieu thanh cong ---')\n",
    "        if X_train.size == 0:\n",
    "            raise ValueError('Du lieu huan luyen tai len bi rong.')\n",
    "        # Kiểm tra số kênh, nếu không khớp thì tạo lại dataset\n",
    "        if X_train.ndim == 4 and X_train.shape[-1] != expected_channels:\n",
    "            raise ValueError(\n",
    "                f'Du lieu cu co {X_train.shape[-1]} kenh, nhung INPUT_SHAPE yeu cau {expected_channels}.'\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Loi khi tai file: {e}. Se thu tao lai.\")\n",
    "        if os.path.exists(PROCESSED_DATA_FULL_PATH):\n",
    "            os.remove(PROCESSED_DATA_FULL_PATH)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = prepare_and_save_data(\n",
    "            DATA_PARENT_DIR,\n",
    "            PROCESSED_DATA_FULL_PATH\n",
    "        )\n",
    "else:\n",
    "    print(f\"--- Khong tim thay du lieu da xu ly. Dang tao moi... ---\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = prepare_and_save_data(\n",
    "        DATA_PARENT_DIR,\n",
    "        PROCESSED_DATA_FULL_PATH\n",
    "    )\n",
    "\n",
    "# --- Kiểm tra, Xử lý và In thông tin Dữ liệu ---\n",
    "if X_train is None or X_train.size == 0:\n",
    "    print('LOI NGHIEM TRONG: Khong co du lieu huan luyen.')\n",
    "    # Dừng thực thi hoặc xử lý lỗi phù hợp\n",
    "else:\n",
    "    print(\n",
    "        f\"\\nKich thuoc du lieu TRUOC xu ly: Train={X_train.shape}, \"\n",
    "        f\"Val={X_val.shape if X_val.size > 0 else 'N/A'}, \"\n",
    "        f\"Test={X_test.shape if X_test.size > 0 else 'N/A'}\"\n",
    "    )\n",
    "\n",
    "    train_labels, train_counts = np.unique(y_train.astype(int), return_counts=True)\n",
    "    print(f'Phan bo y_train: {dict(zip(train_labels.tolist(), train_counts.tolist()))}')\n",
    "    if X_val.size > 0:\n",
    "        val_labels, val_counts = np.unique(y_val.astype(int), return_counts=True)\n",
    "        print(f'Phan bo y_val: {dict(zip(val_labels.tolist(), val_counts.tolist()))}')\n",
    "    if X_test.size > 0:\n",
    "        test_labels, test_counts = np.unique(y_test.astype(int), return_counts=True)\n",
    "        print(f'Phan bo y_test: {dict(zip(test_labels.tolist(), test_counts.tolist()))}')\n",
    "\n",
    "\n",
    "    # Kiểm tra NaN/Inf ban đầu\n",
    "    if np.isnan(X_train).any() or np.isinf(X_train).any():\n",
    "        print('!!! CANH BAO: NaN/Inf trong X_train TRUOC chuan hoa !!!')\n",
    "\n",
    "    # Chuẩn hóa\n",
    "    print('\\n--- CHUAN HOA DU LIEU ---')\n",
    "    train_mean = np.mean(X_train, axis=(0, 1, 2), keepdims=True)\n",
    "    train_std = np.std(X_train, axis=(0, 1, 2), keepdims=True)\n",
    "    train_std = np.where(train_std == 0, 1.0, train_std)\n",
    "    train_mean_flat = train_mean.reshape(-1)\n",
    "    train_std_flat = train_std.reshape(-1)\n",
    "    mean_str = ', '.join(f\"{v:.4f}\" for v in train_mean_flat)\n",
    "    std_str = ', '.join(f\"{v:.4f}\" for v in train_std_flat)\n",
    "    print(f'Train mean (per channel): {mean_str}')\n",
    "    print(f'Train std (per channel): {std_str}')\n",
    "    X_train = (X_train - train_mean) / train_std\n",
    "    if X_val.size > 0:\n",
    "        X_val = (X_val - train_mean) / train_std\n",
    "    if X_test.size > 0:\n",
    "        X_test = (X_test - train_mean) / train_std\n",
    "    print('Da ap dung chuan hoa.')\n",
    "\n",
    "    # Kiểm tra Min/Max sau chuẩn hóa\n",
    "    print('\\n--- KIEM TRA MIN/MAX (SAU CHUAN HOA) ---')\n",
    "    print(f'X_train Min/Max: {np.min(X_train):.4f} / {np.max(X_train):.4f}')\n",
    "    if X_val.size > 0:\n",
    "        print(f'X_val Min/Max: {np.min(X_val):.4f} / {np.max(X_val):.4f}')\n",
    "    if np.isnan(X_train).any() or np.isinf(X_train).any():\n",
    "        print('!!! CANH BAO: NaN/Inf trong X_train SAU chuan hoa !!!')\n",
    "\n",
    "    # Ép kiểu float32\n",
    "    print('\\n--- EP KIEU DU LIEU SANG float32 ---')\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    if X_val.size > 0:\n",
    "        X_val, y_val = X_val.astype(np.float32), y_val.astype(np.float32)\n",
    "    if X_test.size > 0:\n",
    "        X_test, y_test = X_test.astype(np.float32), y_test.astype(np.float32)\n",
    "    print('Da ep kieu sang float32.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T17:41:20.893176Z",
     "iopub.status.busy": "2025-10-29T17:41:20.892503Z",
     "iopub.status.idle": "2025-10-29T17:41:20.907315Z",
     "shell.execute_reply": "2025-10-29T17:41:20.906353Z",
     "shell.execute_reply.started": "2025-10-29T17:41:20.893156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- BUOC HUAN LUYEN MODEL ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy, Precision, Recall\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "tf.config.optimizer.set_jit(False)\n",
    "print('--- Da tat toi uu hoa XLA JIT ---')\n",
    "tf.debugging.enable_check_numerics()\n",
    "print('*** Da bat tf.debugging.enable_check_numerics() ***')\n",
    "\n",
    "def _clean_array_in_place(name, arr):\n",
    "    if arr is None or not isinstance(arr, np.ndarray):\n",
    "        return\n",
    "    nan_count = int(np.isnan(arr).sum())\n",
    "    inf_count = int(np.isinf(arr).sum())\n",
    "    if nan_count or inf_count:\n",
    "        print(f\"CANH BAO: {name} co {nan_count} NaN va {inf_count} Inf. Thay the bang 0.\")\n",
    "    np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0, copy=False)\n",
    "    print(f\"  {name}: shape={arr.shape}, dtype={arr.dtype}, min={arr.min():.4f}, max={arr.max():.4f}\")\n",
    "\n",
    "EPOCHS = 180\n",
    "print(f\"--- So epochs: {EPOCHS} ---\")\n",
    "\n",
    "available_gpus = tf.config.list_physical_devices('GPU')\n",
    "if available_gpus:\n",
    "    gpu_names = [gpu.name for gpu in available_gpus]\n",
    "    print(f\"Phat hien {len(available_gpus)} GPU vat ly: {gpu_names}\")\n",
    "else:\n",
    "    print('Khong tim thay GPU vat ly. Huon luyen tren CPU hoac TPU neu co.')\n",
    "\n",
    "if len(available_gpus) > 1:\n",
    "    print('Khoi tao tf.distribute.MirroredStrategy de tan dung nhieu GPU.')\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(f\"Dang su dung chien luoc: {strategy.__class__.__name__}\")\n",
    "num_replicas = strategy.num_replicas_in_sync\n",
    "print(f\"So luong thiet bi (GPU logic): {num_replicas}\")\n",
    "\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * max(1, num_replicas)\n",
    "if num_replicas > 1:\n",
    "    print(f\"Batch size moi replica: {BATCH_SIZE}, batch size toan cuc: {GLOBAL_BATCH_SIZE}\")\n",
    "else:\n",
    "    print(f\"Batch size su dung: {GLOBAL_BATCH_SIZE}\")\n",
    "\n",
    "print('Khoi tao Data Generator (SpecAugmentSequence)')\n",
    "TIME_MASK_PARAM_T_GEN = 12\n",
    "NUM_TIME_MASKS_GEN = 1\n",
    "FREQ_MASK_PARAM_F_GEN = 12\n",
    "NUM_FREQ_MASKS_GEN = 1\n",
    "AUGMENT_PROB_GEN = 0.12\n",
    "NOISE_STD_GEN = 0.002\n",
    "GAIN_RANGE_DB = (-1.0, 1.0)\n",
    "\n",
    "train_data_generator = None\n",
    "val_data_for_fit = None\n",
    "\n",
    "for name in ('X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test'):\n",
    "    if name in globals():\n",
    "        _clean_array_in_place(name, globals()[name])\n",
    "    else:\n",
    "        print(f\"  {name}: khong ton tai trong bo nho.\")\n",
    "\n",
    "if 'X_train' in locals() and isinstance(X_train, np.ndarray) and X_train.size > 0:\n",
    "    train_data_generator = SpecAugmentSequence(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        GLOBAL_BATCH_SIZE,\n",
    "        apply_time_mask=True,\n",
    "        time_mask_param_T=TIME_MASK_PARAM_T_GEN,\n",
    "        num_time_masks=NUM_TIME_MASKS_GEN,\n",
    "        apply_freq_mask=True,\n",
    "        freq_mask_param_F=FREQ_MASK_PARAM_F_GEN,\n",
    "        num_freq_masks=NUM_FREQ_MASKS_GEN,\n",
    "        apply_noise=True,\n",
    "        noise_std=NOISE_STD_GEN,\n",
    "        apply_gain=True,\n",
    "        gain_db_range=GAIN_RANGE_DB,\n",
    "        augment_prob=AUGMENT_PROB_GEN,\n",
    "        shuffle=True\n",
    "    )\n",
    "    print(f\"Da tao Data Generator voi Batch Size = {GLOBAL_BATCH_SIZE}, augment_prob = {AUGMENT_PROB_GEN}\")\n",
    "    sample_x, sample_y = train_data_generator[0]\n",
    "    # Random batch sanity check\n",
    "    if len(train_data_generator) > 1:\n",
    "        rand_idx = np.random.randint(0, len(train_data_generator))\n",
    "        rand_x, _ = train_data_generator[rand_idx]\n",
    "        rand_x = np.nan_to_num(rand_x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        if not np.isfinite(rand_x).all():\n",
    "            raise ValueError('Batch nga u nhien co NaN/Inf sau augment (rand batch).')\n",
    "    sample_x = sample_x.astype(np.float32, copy=False)\n",
    "    sample_x = np.nan_to_num(sample_x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    finite_ratio = np.mean(np.isfinite(sample_x))\n",
    "    print(f'  Kiem tra batch dau tien: ti le gia tri huu han = {finite_ratio:.4f}, min = {np.min(sample_x):.4f}, max = {np.max(sample_x):.4f}')\n",
    "    channel_means = sample_x.mean(axis=(0, 1, 2))\n",
    "    channel_stds = sample_x.std(axis=(0, 1, 2))\n",
    "    channel_report = ', '.join([f'ch{i}: mean={channel_means[i]:.4f}, std={channel_stds[i]:.4f}' for i in range(len(channel_means))])\n",
    "    print(f'  {channel_report}')\n",
    "    if finite_ratio < 1.0:\n",
    "        raise ValueError('Batch dau tien co NaN/Inf sau augment')\n",
    "    if 'X_val' in locals() and isinstance(X_val, np.ndarray) and X_val.size > 0:\n",
    "        val_data_for_fit = (X_val, y_val)\n",
    "        print('Su dung (X_val, y_val) cho validation_data.')\n",
    "else:\n",
    "    print('LOI: Khong co du lieu X_train de tao Data Generator.')\n",
    "\n",
    "if 'y_train' in locals() and isinstance(y_train, np.ndarray) and y_train.size > 0:\n",
    "    train_clean_ratio = float(np.mean(y_train == 0))\n",
    "    print(f\"Ti le lop (train) - sach: {train_clean_ratio:.3f}, stego: {1 - train_clean_ratio:.3f}\")\n",
    "if 'y_val' in locals() and isinstance(y_val, np.ndarray) and y_val.size > 0:\n",
    "    val_clean_ratio = float(np.mean(y_val == 0))\n",
    "    print(f\"Ti le lop (val)   - sach: {val_clean_ratio:.3f}, stego: {1 - val_clean_ratio:.3f}\")\n",
    "if 'y_test' in locals() and isinstance(y_test, np.ndarray) and y_test.size > 0:\n",
    "    test_clean_ratio = float(np.mean(y_test == 0))\n",
    "    print(f\"Ti le lop (test)  - sach: {test_clean_ratio:.3f}, stego: {1 - test_clean_ratio:.3f}\")\n",
    "\n",
    "class_weights = None\n",
    "print(f\"Su dung Class Weights: {class_weights}\")\n",
    "print('  -> Khong dung class weight (None) de tranh bias.')\n",
    "\n",
    "with strategy.scope():\n",
    "    print('Xay dung Mo hinh CNN (relu, BN, multi-channel)')\n",
    "    model = build_cnn_model_with_bn(INPUT_SHAPE)\n",
    "\n",
    "    print('Cau hinh Optimizer (Adam) va Compile Model')\n",
    "    initial_learning_rate = 2e-4\n",
    "    if train_data_generator is not None:\n",
    "        steps_per_epoch = len(train_data_generator)\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate,\n",
    "            clipnorm=1.0\n",
    "        )\n",
    "        print(f\"Su dung Adam voi learning_rate co dinh = {initial_learning_rate} (steps_per_epoch={steps_per_epoch}).\")\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate, clipnorm=1.0)\n",
    "        print(f\"CANH BAO: Khong co du lieu huan luyen, su dung Adam LR={initial_learning_rate}.\")\n",
    "\n",
    "    loss_fn = BinaryCrossentropy()\n",
    "    print('Ap dung BinaryCrossentropy khong label smoothing.')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            BinaryAccuracy(name='accuracy', threshold=0.5),\n",
    "            Precision(name='precision', thresholds=0.5),\n",
    "            Recall(name='recall', thresholds=0.5),\n",
    "            AUC(name='auc'),\n",
    "            AUC(name='pr_auc', curve='PR')\n",
    "        ]\n",
    "    )\n",
    "    print('Da compile model voi cac metric: accuracy, precision, recall, auc, pr_auc.')\n",
    "\n",
    "model_checkpoint_path = os.path.join(MODEL_SAVE_DIR, model_checkpoint_filename)\n",
    "\n",
    "callbacks_list = []\n",
    "callbacks_list.append(TerminateOnNaN())\n",
    "if val_data_for_fit:\n",
    "    callbacks_list.append(\n",
    "        EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            patience=28,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    print(\"Da them EarlyStopping (monitor=val_auc, patience=28).\")\n",
    "else:\n",
    "    print('Khong co du lieu validation, bo qua EarlyStopping.')\n",
    "\n",
    "callbacks_list.append(\n",
    "    ModelCheckpoint(\n",
    "        model_checkpoint_path,\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    ")\n",
    "\n",
    "if val_data_for_fit:\n",
    "    callbacks_list.append(\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            min_lr=5e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"BAT DAU QUA TRINH HUAN LUYEN (multi-channel, Adam lr={initial_learning_rate}, augment={AUGMENT_PROB_GEN}, {EPOCHS} epochs, class_weights={class_weights})\")\n",
    "history = None\n",
    "fit_kwargs = {}\n",
    "if class_weights is not None:\n",
    "    fit_kwargs['class_weight'] = class_weights\n",
    "if train_data_generator:\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            train_data_generator,\n",
    "            epochs=EPOCHS,\n",
    "            validation_data=val_data_for_fit,\n",
    "            callbacks=callbacks_list,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        print('--- HUAN LUYEN HOAN TAT ---')\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        print(f\"LOI InvalidArgumentError KHI HUAN LUYEN: {e}\")\n",
    "else:\n",
    "    print('HUAN LUYEN BI HUY DO KHONG TAO DUOC Data Generator')\n",
    "\n",
    "if history is not None:\n",
    "    print('--- QUA TRINH HOAN TAT ---')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-29T17:41:20.907873Z",
     "iopub.status.idle": "2025-10-29T17:41:20.908135Z",
     "shell.execute_reply": "2025-10-29T17:41:20.907995Z",
     "shell.execute_reply.started": "2025-10-29T17:41:20.907985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- BUOC DANH GIA VA VE BIEU DO ---\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc as sklearn_auc,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "CUSTOM_DECISION_THRESHOLD = None  # Dat None neu muon dung nguong F1 toi uu\n",
    "THRESHOLD_CANDIDATES = [0.10, 0.15, 0.20, 0.25, 0.30, 0.32, 0.35, 0.40, 0.45]\n",
    "ROC_PLOT_FILENAME = f'roc_curve_val_{run_id}.png'\n",
    "\n",
    "def summary_probability_stats(name, probs):\n",
    "    probs = np.nan_to_num(np.asarray(probs).ravel(), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if probs.size == 0:\n",
    "        print(f'{name}: khong co du lieu de thong ke.')\n",
    "        return probs\n",
    "    print(f'Thong ke xac suat ({name}):')\n",
    "    print(f'  Mean={probs.mean():.4f}, Std={probs.std():.4f}, Min={probs.min():.4f}, Max={probs.max():.4f}')\n",
    "    hist_vals, hist_edges = np.histogram(probs, bins=np.linspace(0.0, 1.0, 11))\n",
    "    for idx, count in enumerate(hist_vals):\n",
    "        print(f'  {hist_edges[idx]:.1f}-{hist_edges[idx + 1]:.1f}: {int(count)}')\n",
    "    return probs\n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "ensure_dir(RESULT_IMAGE_DIR)\n",
    "\n",
    "print()\n",
    "print(f\"--- Tai model tu '{model_checkpoint_path}' de danh gia ---\")\n",
    "best_model_to_evaluate = None\n",
    "current_strategy = tf.distribute.get_strategy()\n",
    "if os.path.exists(model_checkpoint_path):\n",
    "    try:\n",
    "        with current_strategy.scope():\n",
    "            best_model_to_evaluate = tf.keras.models.load_model(model_checkpoint_path)\n",
    "        print('Da load model tot nhat tu checkpoint.')\n",
    "    except Exception as exc:  # pylint: disable=broad-except\n",
    "        print(f'Loi load checkpoint: {exc}')\n",
    "        if 'model' in locals():\n",
    "            best_model_to_evaluate = model\n",
    "            print('Su dung model cuoi cung tren bo nho thay the.')\n",
    "else:\n",
    "    if 'model' in locals():\n",
    "        best_model_to_evaluate = model\n",
    "        print('Khong thay checkpoint, su dung model cuoi cung.')\n",
    "    else:\n",
    "        print('Khong co model nao de danh gia.')\n",
    "\n",
    "best_threshold_f1 = 0.5\n",
    "val_probs = None\n",
    "\n",
    "if best_model_to_evaluate is not None and 'X_val' in locals() and isinstance(X_val, np.ndarray) and X_val.size > 0:\n",
    "    print()\n",
    "    print('--- Danh gia tren tap Validation ---')\n",
    "    val_probs = summary_probability_stats('Validation', best_model_to_evaluate.predict(X_val))\n",
    "    precisions_pr, recalls_pr, thresholds_pr = precision_recall_curve(y_val, val_probs)\n",
    "    thresholds_from_pr = thresholds_pr if thresholds_pr.size > 0 else np.array([])\n",
    "    candidate_thresholds_val = np.unique(\n",
    "        np.concatenate([\n",
    "            thresholds_from_pr,\n",
    "            np.asarray(THRESHOLD_CANDIDATES, dtype=np.float32),\n",
    "            np.array([0.5], dtype=np.float32)\n",
    "        ])\n",
    "    )\n",
    "    candidate_thresholds_val = candidate_thresholds_val[np.isfinite(candidate_thresholds_val)]\n",
    "\n",
    "    best_threshold_macro = 0.5\n",
    "    best_macro_f1 = -1.0\n",
    "    best_macro_details = None\n",
    "\n",
    "    best_threshold_positive = 0.5\n",
    "    best_positive_f1 = -1.0\n",
    "    best_positive_details = None\n",
    "\n",
    "    for thr in candidate_thresholds_val:\n",
    "        preds_thr = (val_probs >= thr).astype(int)\n",
    "        precs_thr = precision_score(y_val, preds_thr, average=None, labels=[0, 1], zero_division=0)\n",
    "        recs_thr = recall_score(y_val, preds_thr, average=None, labels=[0, 1], zero_division=0)\n",
    "        f1s_thr = f1_score(y_val, preds_thr, average=None, labels=[0, 1], zero_division=0)\n",
    "        macro_thr = float(np.mean(f1s_thr))\n",
    "        if macro_thr > best_macro_f1:\n",
    "            best_macro_f1 = macro_thr\n",
    "            best_threshold_macro = float(thr)\n",
    "            best_macro_details = (precs_thr, recs_thr, f1s_thr)\n",
    "        if f1s_thr[1] > best_positive_f1:\n",
    "            best_positive_f1 = float(f1s_thr[1])\n",
    "            best_threshold_positive = float(thr)\n",
    "            best_positive_details = (precs_thr[1], recs_thr[1], f1s_thr[1])\n",
    "\n",
    "    if best_macro_details is not None:\n",
    "        precs_macro, recs_macro, f1s_macro = best_macro_details\n",
    "        print(\n",
    "            f\"Nguong toi uu theo macro F1: {best_threshold_macro:.4f} \"\n",
    "            f\"(F1_clean={f1s_macro[0]:.4f}, F1_stego={f1s_macro[1]:.4f}, macro={best_macro_f1:.4f})\"\n",
    "        )\n",
    "    else:\n",
    "        print('Khong tinh duoc macro F1 hop le, giu nguong 0.5.')\n",
    "        best_threshold_macro = 0.5\n",
    "        best_macro_f1 = 0.0\n",
    "\n",
    "    if best_positive_details is not None:\n",
    "        prec_pos, rec_pos, f1_pos = best_positive_details\n",
    "        print(f\"Nguong toi uu theo F1 lop stego: {best_threshold_positive:.4f} (precision={prec_pos:.4f}, recall={rec_pos:.4f}, F1={f1_pos:.4f})\")\n",
    "    else:\n",
    "        print('Khong tinh duoc F1 cho lop stego, giu nguong 0.5.')\n",
    "        best_threshold_positive = 0.5\n",
    "        best_positive_f1 = 0.0\n",
    "\n",
    "    best_threshold_f1 = best_threshold_macro\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(recalls_pr, precisions_pr, label='PR Curve')\n",
    "    if thresholds_pr.size > 0 and len(recalls_pr) > 0:\n",
    "        try:\n",
    "            close_idx = np.argmin(np.abs(thresholds_pr - best_threshold_f1))\n",
    "            point_idx = min(close_idx + 1, len(recalls_pr) - 1)\n",
    "            plt.scatter(recalls_pr[point_idx], precisions_pr[point_idx], color='red', s=80, label=f'Macro F1 (thr={best_threshold_f1:.2f})')\n",
    "        except Exception as exc:  # pylint: disable=broad-except\n",
    "            print(f'Khong ve duoc diem nguong macro: {exc}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (Validation)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    pr_path = os.path.join(RESULT_IMAGE_DIR, pr_curve_filename)\n",
    "    plt.savefig(pr_path)\n",
    "    plt.close()\n",
    "    print(f'Da luu PR curve: {pr_path}')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_val, val_probs)\n",
    "    roc_auc = sklearn_auc(fpr, tpr)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC (AUC={roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='grey')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve (Validation)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    roc_path = os.path.join(RESULT_IMAGE_DIR, ROC_PLOT_FILENAME)\n",
    "    plt.savefig(roc_path)\n",
    "    plt.close()\n",
    "    print(f'Da luu ROC curve: {roc_path}')\n",
    "else:\n",
    "    print('Khong co du lieu Validation hoac model de tinh PR/ROC.')\n",
    "\n",
    "candidate_thresholds = sorted(set(THRESHOLD_CANDIDATES + [best_threshold_macro, best_threshold_positive]))\n",
    "if CUSTOM_DECISION_THRESHOLD is not None:\n",
    "    candidate_thresholds.append(CUSTOM_DECISION_THRESHOLD)\n",
    "candidate_thresholds = sorted(set(candidate_thresholds))\n",
    "\n",
    "if best_model_to_evaluate is not None and 'X_test' in locals() and isinstance(X_test, np.ndarray) and X_test.size > 0:\n",
    "    print()\n",
    "    print('--- Danh gia tren tap Test ---')\n",
    "    test_probs = summary_probability_stats('Test', best_model_to_evaluate.predict(X_test))\n",
    "\n",
    "    def summarize_threshold(threshold, probs, labels):\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        precs = precision_score(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "        recs = recall_score(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "        f1s = f1_score(labels, preds, average=None, labels=[0, 1], zero_division=0)\n",
    "        cm = confusion_matrix(labels, preds, labels=[0, 1])\n",
    "        return precs, recs, f1s, cm\n",
    "\n",
    "    print('Threshold | P_clean | R_clean | F1_clean | P_stego | R_stego | F1_stego')\n",
    "    print('---------|---------|---------|---------|---------|---------|---------')\n",
    "    threshold_summaries = {}\n",
    "    for thr in candidate_thresholds:\n",
    "        precs_thr, recs_thr, f1s_thr, cm_thr = summarize_threshold(thr, test_probs, y_test)\n",
    "        threshold_summaries[thr] = {\n",
    "            'precision': precs_thr,\n",
    "            'recall': recs_thr,\n",
    "            'f1': f1s_thr,\n",
    "            'cm': cm_thr,\n",
    "        }\n",
    "        print(\n",
    "            f'{thr:9.4f}| {precs_thr[0]:.4f} | {recs_thr[0]:.4f} | {f1s_thr[0]:.4f} | '\n",
    "            f'{precs_thr[1]:.4f} | {recs_thr[1]:.4f} | {f1s_thr[1]:.4f}'\n",
    "        )\n",
    "\n",
    "    if CUSTOM_DECISION_THRESHOLD is not None:\n",
    "        final_threshold = CUSTOM_DECISION_THRESHOLD\n",
    "        print()\n",
    "        print(f'Su dung nguong tuy chinh: {final_threshold:.4f}')\n",
    "    else:\n",
    "        final_threshold = best_threshold_macro\n",
    "        print()\n",
    "        print(f'Su dung nguong toi uu theo macro F1 tren Validation: {final_threshold:.4f}')\n",
    "        print(f' (Nguong F1 lop stego: {best_threshold_positive:.4f})')\n",
    "\n",
    "    final_preds = (test_probs >= final_threshold).astype(int)\n",
    "    print()\n",
    "    print('Classification Report (Test):')\n",
    "    print(classification_report(y_test, final_preds, target_names=['Lop Sach (0)', 'Lop Stego (1)'], digits=4, zero_division=0))\n",
    "    cm_final = confusion_matrix(y_test, final_preds, labels=[0, 1])\n",
    "    print('Confusion Matrix (Test):')\n",
    "    print(cm_final)\n",
    "    fp = int(cm_final[0, 1])\n",
    "    fn = int(cm_final[1, 0])\n",
    "    print(f'False Positive (Sach->Stego): {fp}, False Negative (Stego->Sach): {fn}')\n",
    "else:\n",
    "    print('Khong co du lieu Test hoac model de danh gia.')\n",
    "\n",
    "print()\n",
    "print('--- Ve do thi lich su huan luyen ---')\n",
    "if 'history' in locals() and history is not None and hasattr(history, 'history') and history.history:\n",
    "    metrics_to_plot = ['accuracy', 'loss', 'precision', 'recall', 'auc', 'pr_auc']\n",
    "    available = [m for m in metrics_to_plot if m in history.history]\n",
    "    if available:\n",
    "        rows = (len(available) + 1) // 2\n",
    "        plt.figure(figsize=(15, 5 * rows))\n",
    "        for idx, metric in enumerate(available, start=1):\n",
    "            plt.subplot(rows, 2, idx)\n",
    "            plt.plot(history.history[metric], label=f'Train {metric}')\n",
    "            val_key = f'val_{metric}'\n",
    "            if val_key in history.history:\n",
    "                plt.plot(history.history[val_key], label=f'Val {metric}')\n",
    "            plt.title(metric.capitalize())\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        hist_path = os.path.join(RESULT_IMAGE_DIR, training_history_plot_filename)\n",
    "        plt.savefig(hist_path)\n",
    "        plt.close()\n",
    "        print(f'Da luu bieu do training: {hist_path}')\n",
    "    else:\n",
    "        print('History khong co metric nao hop le de ve.')\n",
    "else:\n",
    "    print('Khong co history (co the do training bi huy truoc do).')\n",
    "\n",
    "print()\n",
    "print('--- HOAN TAT BUOC DANH GIA ---')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8570382,
     "sourceId": 13498258,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
